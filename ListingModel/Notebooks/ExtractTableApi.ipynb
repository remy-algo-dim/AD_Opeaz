{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f5744333",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ExtractTable import ExtractTable\n",
    "import os\n",
    "import random\n",
    "import ast\n",
    "from Levenshtein import distance as lev\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import pandas as pd\n",
    "import logging\n",
    "import sys\n",
    "import pytesseract\n",
    "import re\n",
    "\n",
    "logging.getLogger(\"requests\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "5e497395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logger\n",
    "logging.basicConfig(stream=sys.stdout,\n",
    "                    level=logging.DEBUG,\n",
    "                    format='%(asctime)s %(name)-12s %(levelname)-8s %(lineno)d %(message)s')\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee33f4f",
   "metadata": {},
   "source": [
    "### 0 - Mapping labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "e2cef4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping: key: labels qu'on souhaite retourner, values: toutes les typo qu'on retrouve dans les documents\n",
    "# final_inversed_mapping: plus utile pour faire une for loop lorsqu'on effectuera le mapping\n",
    "\n",
    "mapping = {\n",
    "\t\n",
    "\t\t\"reference\": '[\"Code13Réf\", \"Code Produit\", \"CIP\", \"Code CIP\", \"Code\", \\\n",
    "        \"Gamme GAVISCONELL Désignation (CIP)\", \"Code produit\", \"Code produit (ou Code LPP)\t\", \\\n",
    "    \"Code Article\", \"C.I.P / Article\", \"Code EAN\"]',\n",
    "\t\t\"quantite\": '[\"Qté\", \"Qté Vendue\", \"Nb unité\", \"Qt\", \"Qte Facturée\", \"Qte Délivrée\", \"quantité\", \\\n",
    "    \"Quantité\", \"Qt é\", \"Nombre Unités\", \"Nombre\", \"Quantité Lieu\", \"Cdée\", \"Qt e\", \"Unités\"]',\n",
    "\t\t\"CA TTC\": '[\"Montant TTC\", \"CA Brut TTC\", \"CA TTC brut\", \"PV TTC\", \"CA brut TTC\", \"PrixV TTC\", \\\n",
    "    \"Ch Affaire T.T.C.\", \"CA TTC\", ]',\n",
    "\t\t\"CA HT\": '[\"Rémunération HT\", \"Chiffre Affraire HT\", \"Chiffre\", \"Ch Affaire H.T.\", \"CA HT\", \\\n",
    "    \"Montant cdé\", \"Valeur\", \"Montant HT\", ]',\n",
    "        \"CA\": '[\"CA\"]',\n",
    "\t\t\"prix unitaire HT\": '[\"Prix Vente\", \"Prix Unitaire €\", \"Px vente\", \\\n",
    "    \"PU HT\", \"HT U Brut\", \"Prix HT unitaire\", \"Prix unitaire HT\", ]',\n",
    "    \"prix unitaire TTC\": '[\"Prix TTC\", \"Prix Vente TTC\", \"PVTTC REMISE\"]',\n",
    "          }\n",
    "\n",
    "inversed_mapping = {value: key for key, value in mapping.items()}\n",
    "\n",
    "final_inversed_mapping = {}\n",
    "for key, value in inversed_mapping.items():\n",
    "    key = ast.literal_eval(key)\n",
    "    if type(key) == list:\n",
    "        for item in key:\n",
    "            final_inversed_mapping[item] = value\n",
    "    else:\n",
    "        final_inversed_mapping[key] = value\n",
    "\n",
    "# all_possible_elements is a list with all possible labels discovered in the docs\n",
    "all_possible_elements = [ast.literal_eval(l) for l in mapping.values()]\n",
    "all_possible_elements = [item for sublist in all_possible_elements for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0b72aa",
   "metadata": {},
   "source": [
    "# ExtractTable API\n",
    "Cet API Python permet d'extraire magnifiquement bien un tableau d'un document PDF\n",
    "Raphael Zeitoun a pris 5.000 crédits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d802518a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'credits': 5000, 'queued': 0, 'used': 632}\n"
     ]
    }
   ],
   "source": [
    "api_key = \"sJCGEoGgykvttxEq8bN6XJTzjWukOVIeL0jULE8p\"\n",
    "et_sess = ExtractTable(api_key)\n",
    "print(et_sess.check_usage())        # Checks the API Key validity as well as shows associated plan usage "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae83a50",
   "metadata": {},
   "source": [
    "### 1 - Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e4ddeb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_checks_after_API_and_cleaning(dict_outputs, filename_pdf):\n",
    "    \"\"\"\n",
    "    Input: Dict output after cleaning\n",
    "    Output: new dict with sanity check\n",
    "    \"\"\"\n",
    "    # Need to use the df after cleaning name\n",
    "    df_outputs = pd.DataFrame(dict_outputs)\n",
    "    \n",
    "    # 1 - Check that reference number contains 13 digits\n",
    "    if \"reference\" in df_outputs.columns.tolist():\n",
    "        mask = (df_outputs['reference'].str.replace(' ', '').str.len() == 13)\n",
    "        df_outputs = df_outputs.loc[mask]\n",
    "    else:\n",
    "        logger.info(\"reference not detected\")\n",
    "        reference_pytesseract = pytesseract_for_ref(filename_pdf)\n",
    "        df_outputs[\"reference\"] = [reference_pytesseract for i in range(len(df_outputs))]\n",
    "    \n",
    "    return df_outputs\n",
    "\n",
    "\n",
    "def sanity_check_before_API(file):\n",
    "    \"\"\"\n",
    "    Input: file name\n",
    "    Output: elements that the API will missed\n",
    "    \"\"\"\n",
    "    # 3 - Objectif: Si reference manque:\n",
    "    reference = fix_special_cases(file)\n",
    "    return reference\n",
    "\n",
    "\n",
    "def sanity_check_just_after_API(df_outputs):\n",
    "    # Need to use the df before cleaning name\n",
    "    \n",
    "    # 1 - Objectif: choisir les bonnes colonnes\n",
    "    # parfois certains noms de colonnes sont des duplicates ... exemple (proofs-picture_proof-6962_la pharmatheque ratarieux-listing_listerine.pdf)\n",
    "    duplicates_columns = list(set([col for col in df_outputs.columns.tolist() if df_outputs.columns.tolist().count(col) > 1]))\n",
    "    if len(duplicates_columns) > 0:\n",
    "        logger.info(\"Duplicates columns detected\")\n",
    "        df_outputs = df_outputs.iloc[:, 0:4] # on garde les 3 premieres colonnes. Cette technique est hardcoée sur le document exemple\n",
    "       \n",
    "    return df_outputs, df_outputs.to_dict()\n",
    "\n",
    "\n",
    "def fix_special_cases(file):\n",
    "    # 1 - Objectif: reperer la reference du produit\n",
    "    # Reference produit en titre (exemple: proofs-picture_proof-9439_PHARMACIE DE L ELIXIR-eludril.pdf)\n",
    "    # TODO: comment detecter qu'il s'agit de ce type de document ? Appliquer tesseract en premier ?\n",
    "    # commencer par ExtractTable, voir qu'il manque la ref, puis passer a tesseract et reutiliser ExtractTable ?\n",
    "    # Utilisons le nom du doc pour l'instant:\n",
    "    if 'PHARMACIE DE L ELIXIR' in file:\n",
    "        logger.info('PHARMACIE DE L ELIXIR special case')\n",
    "        text_image = from_pdf_to_jpg(file)\n",
    "        l = [element for element in text_image if element != ' ']\n",
    "        ref = ''\n",
    "        for element in l:\n",
    "            if element.isdigit():\n",
    "                ref += element\n",
    "            else:\n",
    "                ref = ''\n",
    "            if len(ref) == 13:\n",
    "                break\n",
    "        return ref\n",
    "    else:\n",
    "        return None\n",
    "     \n",
    "                              \n",
    "def pytesseract_for_ref(filename_pdf):\n",
    "    pages = convert_from_path(filename_pdf)\n",
    "    for i in range(len(pages)):\n",
    "        file = '{}_page_{}.jpg'.format(filename_pdf[:-4], i)\n",
    "        pages[i].save(file, 'JPEG')\n",
    "    # On lit seulement la page 0 (on ne prend pas en compte les cas ou les pdf > 1 page)\n",
    "    file = '{}_page_{}.jpg'.format(filename_pdf[:-4], 0)\n",
    "    text = pytesseract.image_to_string(file)\n",
    "    # Expression régulière pour les séquences de 13 chiffres\n",
    "    regex = r\"\\d{13}\"\n",
    "    matches = re.findall(regex, s.replace(\" \", \"\"))[0]\n",
    "\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f57b6413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_table(path_image):\n",
    "    \"\"\"\n",
    "    Input: document type A file path (pdf)\n",
    "    Output: print df of retrieved info\n",
    "    PS:To process PDF, make use of pages (\"1\", \"1,3-4\", \"all\") params in the read_pdf function\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    #table_data = et_sess.process_file(filepath=path_image, output_format=\"df\")\n",
    "    table_data = et_sess.process_file(filepath=path_image, output_format=\"df\", pages=\"all\")\n",
    "    et_sess.check_usage()\n",
    "    print(table_data)\n",
    "    \n",
    "    \n",
    "def get_unique_number_pharma(filename):\n",
    "    \"\"\"\n",
    "    Il existe deux types de filenames dans les listing de pharma recus:\n",
    "    type 1: contient le mot deal\n",
    "    type 2: contient le mot proof\n",
    "    En fonction du type, on split d'une certaine façon et réucpère le numéro de la pharmacie, \n",
    "    afin d'appliquer ExtractTable sur un seul numéro à chaque fois, et eviter 100 applications de l'API\n",
    "    si nous avons 100 listings de la même pharma\n",
    "    \"\"\"\n",
    "    if \"deal\" in filename:\n",
    "        number = filename.split(\"_\")[1].split(\"-\")[0]\n",
    "    else:\n",
    "        number = filename.split(\"-\")[2].split(\"_\")[0]\n",
    "    return number\n",
    "\n",
    "\n",
    "def update_retrieved_dictionnary(d, final_inversed_mapping):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    - d: dictionnary retireved by ExtractTable API (via retrieve_info() function)\n",
    "    - final_inversed_mapping: mapping dictionnary which we inversed so that we can do the mapping\n",
    "    Output:\n",
    "    cleaned dict\n",
    "    \"\"\"\n",
    "\n",
    "    keys2rename = []\n",
    "    values2add = []\n",
    "    keysNotFound = []\n",
    "    \n",
    "    # 1. Mapping\n",
    "    for key, value in d.items():\n",
    "        if key in list(final_inversed_mapping.keys()):\n",
    "            keys2rename.append(key)\n",
    "            values2add.append(value)\n",
    "        else:\n",
    "            keysNotFound.append(key)\n",
    "    \n",
    "    # 2. For renaming the keys with the correct ones, we need to remove the old ones and add the new ones\n",
    "    # 2.1 Remove old\n",
    "    for key in keys2rename:\n",
    "        del d[key]\n",
    "    \n",
    "    # 2.2 Add new\n",
    "    for key, value in zip(keys2rename, values2add):\n",
    "        d[final_inversed_mapping[key]] = value\n",
    "    \n",
    "    # 3. Remove the non mapped key (maybe we did not succeeded to match - Tests word similarity)\n",
    "    for key in keysNotFound:\n",
    "        del d[key]\n",
    "    return d\n",
    "\n",
    "\n",
    "def retrieve_info(path_image, pp):\n",
    "    \"\"\"\n",
    "    Input: document type A file path (pdf)\n",
    "    pp: if True, print all the extractions\n",
    "    Output: dictionnary with all info\n",
    "    PS:To process PDF, make use of pages (\"1\", \"1,3-4\", \"all\") params in the read_pdf function\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    #table_data = et_sess.process_file(filepath=path_image, output_format=\"df\")\n",
    "    table_data = et_sess.process_file(filepath=path_image, output_format=\"df\", pages=\"all\")\n",
    "    logger.info(et_sess.check_usage())\n",
    "    if pp:\n",
    "        print(table_data)\n",
    "    df = table_data[0]\n",
    "    df.columns = df.loc[0] # maybe can vary from a doc to another\n",
    "    #df = df[2:] # maybe can vary from a doc to another\n",
    "    return df, df[1:].to_dict(), table_data\n",
    "\n",
    "\n",
    "def pipeline(filename, pp=False, api=True, df_info=False):\n",
    "    \"\"\"\n",
    "    Pipeline\n",
    "    Input: Filename\n",
    "    pp: if True, print all the extractions\n",
    "    api: if True, we apply ExtractTable API. If False, it means that we already applied it and want\n",
    "    to apply the function on the extraced df\n",
    "    df_info: exists only if we already applied the function with api = True the first time\n",
    "    Output: df & dictionnary with the relevant informations (ExtractTable API + mapping)\"\"\"\n",
    "    # Check few things before applying API\n",
    "    logger.info(\"Filename: {} \\n\".format(filename))\n",
    "    reference = sanity_check_before_API(file)\n",
    "    \n",
    "    if api:\n",
    "        df_info, dict_info, table_data = retrieve_info(\"../Data/PDF/{}\".format(filename), pp)\n",
    "    else:\n",
    "        table_data = None\n",
    "    df_info, dict_info = sanity_check_just_after_API(df_info)\n",
    "    print(df_info.head())\n",
    "    \n",
    "    # On a souvent des pb de colonnes, c'est a dire que parfois, les bons noms de colonnes se situent dans les\n",
    "    # lignes, dûs à des noms de colonnes trop longs repérés par l'API. On utilise une while loop dans laquelle\n",
    "    # on remonte les lignes petit à petit jusqu'à ce que le code reconnaisse des noms labels\n",
    "    final_dict = {}\n",
    "    # Tant que le final_dict (output) est vide ou bien tant que la df output n'est pas 1 (ce qui signifie\n",
    "    # qu'il reste encore des lignes à remonter pour les faire devenir columns name, on continue)\n",
    "    i = 1\n",
    "    while len(final_dict) == 0 and len(df_info) != 1:\n",
    "        logger.info(\"Apply function update_retrieved_dictionnary: time {}\".format(i))\n",
    "        final_dict = update_retrieved_dictionnary(dict_info, final_inversed_mapping)\n",
    "        df_info.columns = df_info.loc[0]\n",
    "        df_info = df_info[1:]\n",
    "        dict_info = df_info.to_dict()\n",
    "        df_info.reset_index(drop=True, inplace=True)\n",
    "        i+=1\n",
    "        \n",
    "    if reference:\n",
    "        logger.info(\"reference was spotted outside of the df\")\n",
    "        # TODO verifier sur un exemple si la ligne ce dessous fonctionne\n",
    "        final_dict[\"reference\"] = {i: reference for i in range(len(final_dict.keys()))[0]}\n",
    "    \n",
    "    # Check few things after applying API\n",
    "    final_dict = sanity_checks_after_API_and_cleaning(final_dict, \"../Data/PDF/{}\".format(filename))\n",
    "    return df_info, final_dict, table_data\n",
    "    \n",
    "\n",
    "def from_pdf_to_jpg(file):\n",
    "    \"\"\"Input: pdf file\n",
    "    Ouput: Jpg file, usign tesseract\"\"\"\n",
    "    logger.info(file)\n",
    "    pages = convert_from_path(\"../Data/PDF/{}\".format(file))\n",
    "    for i in range(len(pages)):\n",
    "        pages[i].save('../Data/JPEG/' + file[:-4]+ str(i) +'.jpg', 'JPEG')\n",
    "    # Tesseract\n",
    "    path_image = \"../Data/JPEG/{}\".format(file[:-4]+ str(i) +'.jpg')\n",
    "    text_image = pytesseract.image_to_string(path_image)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "6cada1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_image = \"Data/Exemples/typeB.pdf\"\n",
    "#document_A(path_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0fdfcaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-07 17:53:21,751 urllib3.connectionpool DEBUG    452 https://trigger.extracttable.com:443 \"POST / HTTP/1.1\" 200 1310\n",
      "2023-03-07 17:53:22,286 urllib3.connectionpool DEBUG    452 https://validator.extracttable.com:443 \"GET / HTTP/1.1\" 200 97\n",
      "2023-03-07 17:53:22,289 root         INFO     77 {'credits': 5000, 'queued': 0, 'used': 606}\n"
     ]
    }
   ],
   "source": [
    "#df_info, dict_info, table_data = retrieve_info(\"../Data/PDF/{}\".format(file), False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e44fac",
   "metadata": {},
   "source": [
    "### 3 - Model\n",
    "This is a first baseline model. Lets test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9407425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elemets in 'filenames' list: 2744\n"
     ]
    }
   ],
   "source": [
    "filenames = []\n",
    "for filename in os.listdir(\"../Data/PDF\"):\n",
    "    filenames.append(filename)\n",
    "print(\"Total elemets in 'filenames' list: {}\".format(len(filenames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "454c90b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-08 11:52:03,738 root         INFO     97 Filename: proofs-picture_proof-7699_pharmacie du marché 92250-gallia.pdf \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/remyadda/opt/anaconda3/lib/python3.8/site-packages/ExtractTable/parsers.py:43: UserWarning: Check the status using the JobId after 10 seconds\n",
      "  warnings.warn(_msg_)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info]: Waiting to retrieve the output; JobId: 9d17710cf534edee8b552995915df165581f8eb4d26006401d178986fa28b800\n",
      "2023-03-08 11:52:15,620 root         INFO     78 {'credits': 5000, 'queued': 0, 'used': 636}\n",
      "0                                        CIP LibellO  Janvier  DOcembre  \\\n",
      "0                                        CIP LibellO  Janvier  DOcembre   \n",
      "1  3041091124128 BLEDINER FAR INST LEG POTAGER B/...        0         0   \n",
      "2                 3041091224064 BLEDINER RIZ CAROTTE        0         0   \n",
      "3  3041091474711 GALLIA BB EXP AC TRANSIT 1 LAIT ...        0         1   \n",
      "4  3041091474988 GALLIA BB EXP AC TRANSIT 2 LAIT ...        2         6   \n",
      "\n",
      "0  Novembre  \n",
      "0  Novembre  \n",
      "1         0  \n",
      "2         0  \n",
      "3         6  \n",
      "4         5  \n",
      "2023-03-08 11:52:15,627 root         INFO     115 Apply function update_retrieved_dictionnary: time 1\n",
      "2023-03-08 11:52:15,630 root         INFO     115 Apply function update_retrieved_dictionnary: time 2\n",
      "2023-03-08 11:52:15,633 root         INFO     115 Apply function update_retrieved_dictionnary: time 3\n",
      "2023-03-08 11:52:15,635 root         INFO     115 Apply function update_retrieved_dictionnary: time 4\n",
      "2023-03-08 11:52:15,637 root         INFO     115 Apply function update_retrieved_dictionnary: time 5\n",
      "2023-03-08 11:52:15,640 root         INFO     115 Apply function update_retrieved_dictionnary: time 6\n",
      "2023-03-08 11:52:15,642 root         INFO     115 Apply function update_retrieved_dictionnary: time 7\n",
      "2023-03-08 11:52:15,644 root         INFO     115 Apply function update_retrieved_dictionnary: time 8\n",
      "2023-03-08 11:52:15,646 root         INFO     115 Apply function update_retrieved_dictionnary: time 9\n",
      "2023-03-08 11:52:15,648 root         INFO     115 Apply function update_retrieved_dictionnary: time 10\n",
      "2023-03-08 11:52:15,649 root         INFO     115 Apply function update_retrieved_dictionnary: time 11\n",
      "2023-03-08 11:52:15,651 root         INFO     115 Apply function update_retrieved_dictionnary: time 12\n",
      "2023-03-08 11:52:15,652 root         INFO     115 Apply function update_retrieved_dictionnary: time 13\n",
      "2023-03-08 11:52:15,654 root         INFO     115 Apply function update_retrieved_dictionnary: time 14\n",
      "2023-03-08 11:52:15,656 root         INFO     115 Apply function update_retrieved_dictionnary: time 15\n",
      "2023-03-08 11:52:15,657 root         INFO     115 Apply function update_retrieved_dictionnary: time 16\n",
      "2023-03-08 11:52:15,660 root         INFO     115 Apply function update_retrieved_dictionnary: time 17\n",
      "2023-03-08 11:52:15,662 root         INFO     115 Apply function update_retrieved_dictionnary: time 18\n",
      "2023-03-08 11:52:15,664 root         INFO     115 Apply function update_retrieved_dictionnary: time 19\n",
      "2023-03-08 11:52:15,665 root         INFO     115 Apply function update_retrieved_dictionnary: time 20\n",
      "2023-03-08 11:52:15,668 root         INFO     115 Apply function update_retrieved_dictionnary: time 21\n",
      "2023-03-08 11:52:15,669 root         INFO     115 Apply function update_retrieved_dictionnary: time 22\n",
      "2023-03-08 11:52:15,671 root         INFO     115 Apply function update_retrieved_dictionnary: time 23\n",
      "2023-03-08 11:52:15,672 root         INFO     115 Apply function update_retrieved_dictionnary: time 24\n",
      "2023-03-08 11:52:15,673 root         INFO     115 Apply function update_retrieved_dictionnary: time 25\n",
      "2023-03-08 11:52:15,675 root         INFO     115 Apply function update_retrieved_dictionnary: time 26\n",
      "2023-03-08 11:52:15,676 root         INFO     115 Apply function update_retrieved_dictionnary: time 27\n",
      "2023-03-08 11:52:15,677 root         INFO     115 Apply function update_retrieved_dictionnary: time 28\n",
      "2023-03-08 11:52:15,678 root         INFO     115 Apply function update_retrieved_dictionnary: time 29\n",
      "2023-03-08 11:52:15,680 root         INFO     115 Apply function update_retrieved_dictionnary: time 30\n",
      "2023-03-08 11:52:15,682 root         INFO     115 Apply function update_retrieved_dictionnary: time 31\n",
      "2023-03-08 11:52:15,683 root         INFO     115 Apply function update_retrieved_dictionnary: time 32\n",
      "2023-03-08 11:52:15,685 root         INFO     115 Apply function update_retrieved_dictionnary: time 33\n",
      "2023-03-08 11:52:15,686 root         INFO     115 Apply function update_retrieved_dictionnary: time 34\n",
      "2023-03-08 11:52:15,688 root         INFO     115 Apply function update_retrieved_dictionnary: time 35\n",
      "2023-03-08 11:52:15,690 root         INFO     115 Apply function update_retrieved_dictionnary: time 36\n",
      "2023-03-08 11:52:15,691 root         INFO     115 Apply function update_retrieved_dictionnary: time 37\n",
      "2023-03-08 11:52:15,693 root         INFO     115 Apply function update_retrieved_dictionnary: time 38\n",
      "2023-03-08 11:52:15,695 root         INFO     115 Apply function update_retrieved_dictionnary: time 39\n",
      "2023-03-08 11:52:15,697 root         INFO     14 reference not detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-154-3088dd81e58f>:119: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  dict_info = df_info.to_dict()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-08 11:52:19,050 root         INFO     10 Nothing deteted. Lets test on next table: table 1\n",
      "2023-03-08 11:52:19,052 root         INFO     11 3 tables detected\n",
      "2023-03-08 11:52:19,052 root         INFO     97 Filename: proofs-picture_proof-9160_GRANDE PHARMACIE DE LA CROIX ROUGE-Listing_sortie_Eluday_Plan_Trade_MArs_2022.pdf \n",
      "\n",
      "  0 1 2 3 4 5         6 7  8   9 10  11     12  13\n",
      "0                                    PV  TTC 2  97\n",
      "1              PRODUITS        3     13      3  42\n",
      "2                                    64     21  22\n",
      "3                          0   5     39     20  55\n",
      "4                          1  22     85     22  32\n",
      "2023-03-08 11:52:19,060 root         INFO     115 Apply function update_retrieved_dictionnary: time 1\n",
      "2023-03-08 11:52:19,063 root         INFO     115 Apply function update_retrieved_dictionnary: time 2\n",
      "2023-03-08 11:52:19,066 root         INFO     115 Apply function update_retrieved_dictionnary: time 3\n",
      "2023-03-08 11:52:19,070 root         INFO     115 Apply function update_retrieved_dictionnary: time 4\n",
      "2023-03-08 11:52:19,072 root         INFO     115 Apply function update_retrieved_dictionnary: time 5\n",
      "2023-03-08 11:52:19,077 root         INFO     115 Apply function update_retrieved_dictionnary: time 6\n",
      "2023-03-08 11:52:19,081 root         INFO     115 Apply function update_retrieved_dictionnary: time 7\n",
      "2023-03-08 11:52:19,083 root         INFO     115 Apply function update_retrieved_dictionnary: time 8\n",
      "2023-03-08 11:52:19,085 root         INFO     115 Apply function update_retrieved_dictionnary: time 9\n",
      "2023-03-08 11:52:19,088 root         INFO     115 Apply function update_retrieved_dictionnary: time 10\n",
      "2023-03-08 11:52:19,090 root         INFO     115 Apply function update_retrieved_dictionnary: time 11\n",
      "2023-03-08 11:52:19,093 root         INFO     115 Apply function update_retrieved_dictionnary: time 12\n",
      "2023-03-08 11:52:19,095 root         INFO     115 Apply function update_retrieved_dictionnary: time 13\n",
      "2023-03-08 11:52:19,098 root         INFO     115 Apply function update_retrieved_dictionnary: time 14\n",
      "2023-03-08 11:52:19,101 root         INFO     115 Apply function update_retrieved_dictionnary: time 15\n",
      "2023-03-08 11:52:19,103 root         INFO     115 Apply function update_retrieved_dictionnary: time 16\n",
      "2023-03-08 11:52:19,105 root         INFO     115 Apply function update_retrieved_dictionnary: time 17\n",
      "2023-03-08 11:52:19,108 root         INFO     115 Apply function update_retrieved_dictionnary: time 18\n",
      "2023-03-08 11:52:19,110 root         INFO     115 Apply function update_retrieved_dictionnary: time 19\n",
      "2023-03-08 11:52:19,112 root         INFO     115 Apply function update_retrieved_dictionnary: time 20\n",
      "2023-03-08 11:52:19,115 root         INFO     115 Apply function update_retrieved_dictionnary: time 21\n",
      "2023-03-08 11:52:19,118 root         INFO     115 Apply function update_retrieved_dictionnary: time 22\n",
      "2023-03-08 11:52:19,122 root         INFO     115 Apply function update_retrieved_dictionnary: time 23\n",
      "2023-03-08 11:52:19,124 root         INFO     115 Apply function update_retrieved_dictionnary: time 24\n",
      "2023-03-08 11:52:19,125 root         INFO     115 Apply function update_retrieved_dictionnary: time 25\n",
      "2023-03-08 11:52:19,127 root         INFO     115 Apply function update_retrieved_dictionnary: time 26\n",
      "2023-03-08 11:52:19,129 root         INFO     115 Apply function update_retrieved_dictionnary: time 27\n",
      "2023-03-08 11:52:19,131 root         INFO     115 Apply function update_retrieved_dictionnary: time 28\n",
      "2023-03-08 11:52:19,133 root         INFO     115 Apply function update_retrieved_dictionnary: time 29\n",
      "2023-03-08 11:52:19,136 root         INFO     115 Apply function update_retrieved_dictionnary: time 30\n",
      "2023-03-08 11:52:19,138 root         INFO     115 Apply function update_retrieved_dictionnary: time 31\n",
      "2023-03-08 11:52:19,141 root         INFO     115 Apply function update_retrieved_dictionnary: time 32\n",
      "2023-03-08 11:52:19,144 root         INFO     115 Apply function update_retrieved_dictionnary: time 33\n",
      "2023-03-08 11:52:19,146 root         INFO     115 Apply function update_retrieved_dictionnary: time 34\n",
      "2023-03-08 11:52:19,149 root         INFO     115 Apply function update_retrieved_dictionnary: time 35\n",
      "2023-03-08 11:52:19,152 root         INFO     115 Apply function update_retrieved_dictionnary: time 36\n",
      "2023-03-08 11:52:19,154 root         INFO     115 Apply function update_retrieved_dictionnary: time 37\n",
      "2023-03-08 11:52:19,159 root         INFO     115 Apply function update_retrieved_dictionnary: time 38\n",
      "2023-03-08 11:52:19,161 root         INFO     115 Apply function update_retrieved_dictionnary: time 39\n",
      "2023-03-08 11:52:19,164 root         INFO     115 Apply function update_retrieved_dictionnary: time 40\n",
      "2023-03-08 11:52:19,166 root         INFO     115 Apply function update_retrieved_dictionnary: time 41\n",
      "2023-03-08 11:52:19,168 root         INFO     115 Apply function update_retrieved_dictionnary: time 42\n",
      "2023-03-08 11:52:19,171 root         INFO     115 Apply function update_retrieved_dictionnary: time 43\n",
      "2023-03-08 11:52:19,174 root         INFO     115 Apply function update_retrieved_dictionnary: time 44\n",
      "2023-03-08 11:52:19,177 root         INFO     115 Apply function update_retrieved_dictionnary: time 45\n",
      "2023-03-08 11:52:19,179 root         INFO     14 reference not detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-154-3088dd81e58f>:119: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  dict_info = df_info.to_dict()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-08 11:52:22,497 root         INFO     10 Nothing deteted. Lets test on next table: table 2\n",
      "2023-03-08 11:52:22,499 root         INFO     11 3 tables detected\n",
      "2023-03-08 11:52:22,500 root         INFO     97 Filename: proofs-picture_proof-9160_GRANDE PHARMACIE DE LA CROIX ROUGE-Listing_sortie_Eluday_Plan_Trade_MArs_2022.pdf \n",
      "\n",
      "    0   1\n",
      "0   4  90\n",
      "1   3  90\n",
      "2  18  90\n",
      "3  17  60\n",
      "4  19  20\n",
      "2023-03-08 11:52:22,504 root         INFO     115 Apply function update_retrieved_dictionnary: time 1\n",
      "2023-03-08 11:52:22,507 root         INFO     115 Apply function update_retrieved_dictionnary: time 2\n",
      "2023-03-08 11:52:22,509 root         INFO     115 Apply function update_retrieved_dictionnary: time 3\n",
      "2023-03-08 11:52:22,511 root         INFO     115 Apply function update_retrieved_dictionnary: time 4\n",
      "2023-03-08 11:52:22,513 root         INFO     115 Apply function update_retrieved_dictionnary: time 5\n",
      "2023-03-08 11:52:22,515 root         INFO     115 Apply function update_retrieved_dictionnary: time 6\n",
      "2023-03-08 11:52:22,517 root         INFO     115 Apply function update_retrieved_dictionnary: time 7\n",
      "2023-03-08 11:52:22,519 root         INFO     115 Apply function update_retrieved_dictionnary: time 8\n",
      "2023-03-08 11:52:22,521 root         INFO     115 Apply function update_retrieved_dictionnary: time 9\n",
      "2023-03-08 11:52:22,523 root         INFO     115 Apply function update_retrieved_dictionnary: time 10\n",
      "2023-03-08 11:52:22,525 root         INFO     115 Apply function update_retrieved_dictionnary: time 11\n",
      "2023-03-08 11:52:22,528 root         INFO     115 Apply function update_retrieved_dictionnary: time 12\n",
      "2023-03-08 11:52:22,531 root         INFO     115 Apply function update_retrieved_dictionnary: time 13\n",
      "2023-03-08 11:52:22,533 root         INFO     115 Apply function update_retrieved_dictionnary: time 14\n",
      "2023-03-08 11:52:22,536 root         INFO     115 Apply function update_retrieved_dictionnary: time 15\n",
      "2023-03-08 11:52:22,539 root         INFO     115 Apply function update_retrieved_dictionnary: time 16\n",
      "2023-03-08 11:52:22,542 root         INFO     115 Apply function update_retrieved_dictionnary: time 17\n",
      "2023-03-08 11:52:22,545 root         INFO     115 Apply function update_retrieved_dictionnary: time 18\n",
      "2023-03-08 11:52:22,547 root         INFO     115 Apply function update_retrieved_dictionnary: time 19\n",
      "2023-03-08 11:52:22,549 root         INFO     115 Apply function update_retrieved_dictionnary: time 20\n",
      "2023-03-08 11:52:22,551 root         INFO     115 Apply function update_retrieved_dictionnary: time 21\n",
      "2023-03-08 11:52:22,553 root         INFO     115 Apply function update_retrieved_dictionnary: time 22\n",
      "2023-03-08 11:52:22,557 root         INFO     115 Apply function update_retrieved_dictionnary: time 23\n",
      "2023-03-08 11:52:22,559 root         INFO     115 Apply function update_retrieved_dictionnary: time 24\n",
      "2023-03-08 11:52:22,561 root         INFO     115 Apply function update_retrieved_dictionnary: time 25\n",
      "2023-03-08 11:52:22,564 root         INFO     115 Apply function update_retrieved_dictionnary: time 26\n",
      "2023-03-08 11:52:22,566 root         INFO     115 Apply function update_retrieved_dictionnary: time 27\n",
      "2023-03-08 11:52:22,569 root         INFO     115 Apply function update_retrieved_dictionnary: time 28\n",
      "2023-03-08 11:52:22,572 root         INFO     115 Apply function update_retrieved_dictionnary: time 29\n",
      "2023-03-08 11:52:22,574 root         INFO     115 Apply function update_retrieved_dictionnary: time 30\n",
      "2023-03-08 11:52:22,575 root         INFO     115 Apply function update_retrieved_dictionnary: time 31\n",
      "2023-03-08 11:52:22,577 root         INFO     115 Apply function update_retrieved_dictionnary: time 32\n",
      "2023-03-08 11:52:22,580 root         INFO     115 Apply function update_retrieved_dictionnary: time 33\n",
      "2023-03-08 11:52:22,582 root         INFO     115 Apply function update_retrieved_dictionnary: time 34\n",
      "2023-03-08 11:52:22,584 root         INFO     115 Apply function update_retrieved_dictionnary: time 35\n",
      "2023-03-08 11:52:22,587 root         INFO     115 Apply function update_retrieved_dictionnary: time 36\n",
      "2023-03-08 11:52:22,589 root         INFO     115 Apply function update_retrieved_dictionnary: time 37\n",
      "2023-03-08 11:52:22,591 root         INFO     115 Apply function update_retrieved_dictionnary: time 38\n",
      "2023-03-08 11:52:22,594 root         INFO     14 reference not detected\n",
      "2023-03-08 11:52:25,797 root         INFO     10 Nothing deteted. Lets test on next table: table 3\n",
      "2023-03-08 11:52:25,799 root         INFO     11 3 tables detected\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-156-690ae0ef827e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Nothing deteted. Lets test on next table: table {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} tables detected\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtable_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_table\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mnext_table\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Test the model on some examples\n",
    "file = \"proofs-picture_proof-7699_pharmacie du marché 92250-gallia.pdf\"\n",
    "#file = filenames[12]\n",
    "\n",
    "df, dic, table_data = pipeline(file, pp=False, api=True, df_info=False)\n",
    "\n",
    "next_table = 1\n",
    "\n",
    "while len(dic) == 0:\n",
    "    logger.info(\"Nothing deteted. Lets test on next table: table {}\".format(next_table))\n",
    "    logger.info(\"{} tables detected\".format(len(table_data)))\n",
    "    df, dic, _ = pipeline(filename, pp=False, api=False, df_info=table_data[next_table])\n",
    "    next_table += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aed1091",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e21339",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed52496",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
